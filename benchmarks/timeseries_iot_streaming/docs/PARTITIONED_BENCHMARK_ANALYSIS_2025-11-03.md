# Partitioned Benchmark Analysis - November 3, 2025

**Run ID**: `run_partitioned_20251103_132503`
**Total Runtime**: 28 minutes 21 seconds (1,701 seconds)
**Total Rows**: 29,210,000 per variant (116.84M total across 4 variants)
**Partitions**: 24 per variant (96 total partition tables)

---

## Executive Summary

### ‚úÖ Successes
- **Partition routing**: Perfect distribution across 24 date-based partitions
- **Data integrity**: All variants have identical row counts (29.21M)
- **Phase 1 performance**: PAX at 89.7% of AO speed (expected)
- **Validation framework**: 4 of 5 gates passed

### ‚ùå Critical Findings
1. **Phase 2 did NOT achieve 18-35% speedup target** - Instead showed 37-47% degradation
2. **PAX-no-cluster OUTPERFORMED PAX clustered** in Phase 2 (144K vs 142K rows/sec)
3. **Storage metrics unavailable** - Division by zero errors (fixed in subsequent commit)
4. **Bloom filter cardinality warning** - False positive (columns are 90-91% unique)

### üî¥ Key Conclusion
**Partitioning did NOT deliver the expected 18-35% Phase 2 speedup.** In fact, Phase 2 degradation (37-47%) is WORSE than expected, suggesting partition overhead exceeded index size reduction benefits.

---

## 1. Performance Analysis

### Phase 1: Streaming INSERTs (No Indexes)

| Variant | Throughput (rows/sec) | Total Time | % of AO Speed | Batches |
|---------|----------------------|------------|---------------|---------|
| **AO** | 268,312 | 104.4s | 100.0% (baseline) | 500 |
| **PAX-no-cluster** | 241,571 | 112.3s | 90.0% | 500 |
| **PAX** | 240,628 | 112.8s | 89.7% | 500 |
| **AOCO** | 191,691 | 126.9s | 71.4% | 500 |

**Analysis**:
- ‚úÖ PAX achieves 89.7% of AO speed (consistent with monolithic benchmark findings)
- ‚úÖ PAX and PAX-no-cluster perform identically (clustering not yet applied)
- ‚úÖ AOCO slowest (expected for columnar storage)
- ‚úÖ Min/max throughput ranges show consistency:
  - AO: 196K - 312K rows/sec
  - PAX: 182K - 292K rows/sec
  - AOCO: 106K - 267K rows/sec

**Throughput Variance**:
- AO: 59% range (196K to 312K)
- PAX: 61% range (182K to 292K)
- AOCO: 152% range (106K to 267K) ‚Üê Highest variance

---

### Phase 2: Streaming INSERTs (With Indexes)

| Variant | Throughput (rows/sec) | Total Time | % of AO Speed | Degradation from Phase 1 |
|---------|----------------------|------------|---------------|--------------------------|
| **PAX-no-cluster** | 144,060 | 201.7s | 100.8% | **40.4%** ‚¨áÔ∏è |
| **AO** | 142,879 | 212.9s | 100.0% (baseline) | **46.7%** ‚¨áÔ∏è |
| **PAX** | 142,059 | 205.2s | 99.4% | **41.0%** ‚¨áÔ∏è |
| **AOCO** | 120,402 | 234.5s | 84.3% | **37.2%** ‚¨áÔ∏è |

**üî¥ CRITICAL FINDINGS**:

1. **Phase 2 Degradation WORSE Than Expected**:
   - Expected: 18-35% degradation
   - Actual: 37-47% degradation
   - **Gap**: 2-12 percentage points worse than target

2. **PAX-no-cluster OUTPERFORMED PAX clustered**:
   - PAX-no-cluster: 144,060 rows/sec
   - PAX clustered: 142,059 rows/sec
   - Difference: 1.4% slower (negligible but unexpected direction)
   - **Expected**: PAX clustered should NOT be slower than no-cluster

3. **Variant Ranking Changed**:
   - Phase 1: AO (268K) >> PAX (241K) >> AOCO (192K)
   - Phase 2: PAX-no-cluster (144K) ‚âà AO (143K) ‚âà PAX (142K) >> AOCO (120K)
   - **Observation**: PAX variants "caught up" to AO in Phase 2

4. **Throughput Variance Increased**:
   - PAX-no-cluster: 45K - 189K rows/sec (318% range)
   - AO: 15K - 200K rows/sec (1,216% range) ‚Üê Extreme variance
   - PAX: 16K - 187K rows/sec (1,073% range)
   - AOCO: 12K - 195K rows/sec (1,525% range)

**Min Throughput Analysis** (concerning):
- AO dropped to **15,198 rows/sec** (13x slower than average)
- PAX dropped to **15,873 rows/sec** (9x slower than average)
- AOCO dropped to **12,019 rows/sec** (10x slower than average)

This suggests periodic **index maintenance bottlenecks** (possibly VACUUM, checkpoint, or index bloat).

---

### Phase 1 vs Phase 2 Comparison

| Variant | Phase 1 (rows/sec) | Phase 2 (rows/sec) | Degradation | Expected | Gap |
|---------|-------------------|-------------------|-------------|----------|-----|
| AO | 268,312 | 142,879 | **46.7%** | 18-35% | **+11.7 to +28.7%** |
| AOCO | 191,691 | 120,402 | **37.2%** | 18-35% | **+2.2 to +19.2%** |
| PAX | 240,628 | 142,059 | **41.0%** | 18-35% | **+6.0 to +23.0%** |
| PAX-no-cluster | 241,571 | 144,060 | **40.4%** | 18-35% | **+5.4 to +22.4%** |

**Key Observation**: AOCO has the LOWEST degradation (37.2%), while AO has the WORST (46.7%). This is counter-intuitive if partitioning was expected to help.

---

## 2. Partition Distribution Analysis

### Overall Statistics
- **Total partitions**: 24 per variant (96 total tables)
- **Total rows**: 29,210,000 per variant
- **Average per partition**: 1,217,083 rows
- **Min rows**: 200,000 (partition day00-day04 sample)
- **Max rows**: 2,100,000
- **Skew ratio**: 10.5x (max/min)

### Sample Distribution (First 5 Partitions)
All first 5 partitions have **identical** 210,000 rows:
```
cdr_ao_part_day00: 210,000 rows
cdr_ao_part_day01: 210,000 rows
cdr_ao_part_day02: 210,000 rows
cdr_ao_part_day03: 210,000 rows
cdr_ao_part_day04: 210,000 rows
```

**Analysis**: The first 5 partitions are uniform, but max partition has 2.1M rows (10x more). This suggests:
- Early hours (0-4): Low, consistent traffic (~210K rows/hour)
- Later hours: Much higher traffic (up to 2.1M rows/hour in one partition)
- This is **realistic** for telecom CDR data (peak hours vs off-peak)

---

## 3. Validation Results

### Gate 1: Row Count Consistency ‚úÖ PASSED
- All variants: **29,210,000 rows** (identical)
- 24 partitions per variant
- Average per partition: **1,217,083 rows**

**Verdict**: ‚úÖ Data integrity confirmed across all partitioned variants

---

### Gate 2: PAX Configuration Bloat Check ‚ùå ERROR

**Error**:
```
ERROR:  division by zero
CONTEXT:  PL/pgSQL function inline_code_block line 14 at assignment
```

**Root Cause**: `pg_total_relation_size()` returns 0 for parent partitioned tables in Cloudberry/Greenplum (expected behavior - fixed in commit 1c494f7).

**Impact**: Cannot validate storage bloat, compression ratios, or PAX configuration health.

**Status**: ‚ö†Ô∏è **BLOCKING ISSUE** - Need to re-run metrics with fixed scripts

---

### Gate 3: INSERT Throughput Sanity Check ‚úÖ PASSED

**Phase 1 Throughput**:
- AO: 268,312 rows/sec
- PAX: 240,628 rows/sec
- **Ratio**: PAX is **89.7% of AO speed**

**Threshold**: >50% of AO speed
**Verdict**: ‚úÖ **PASSED** - PAX INSERT speed is acceptable

---

### Gate 4: Bloom Filter Effectiveness üü† WARNING

**Bloom filter column cardinalities**:
- `caller_number`: **0.911781** distinct values (91.2% unique)
- `callee_number`: **0.90979** distinct values (91.0% unique)
- `call_id`: Removed (only 1 distinct value - correctly excluded)

**Warning**:
```
üü† WARNING: Some bloom filter columns have lower cardinality than expected
```

**Analysis**: This is a **FALSE POSITIVE**:
- Cardinalities of 90-91% are **excellent** for bloom filters
- Likely triggered because threshold expects closer to 1.0 (100% unique)
- These columns are appropriate for bloom filter usage

**Recommendation**: Adjust validation threshold in `11b_validate_results_PARTITIONED.sql` to accept >0.85 as healthy.

---

### Gate 5: Partition Health Check ‚úÖ PASSED (with INFO)

**Partition distribution**:
- Total partitions: **24** per variant
- Min rows: **200,000**
- Max rows: **2,100,000**
- Avg rows: **1,217,083**
- **Skew ratio**: 10.5x (max/min)

**Info Message**:
```
üü† INFO: Partition distribution has some skew (max/min >= 2.0)
       - expected for realistic traffic patterns
```

**Verdict**: ‚úÖ **PASSED** - Skew is realistic for CDR data (peak vs off-peak hours)

---

## 4. Root Cause Analysis: Why Did Partitioning NOT Deliver Expected Speedup?

### Expected Benefits (from OPTIMIZATION_TESTING_PLAN.md)
1. **Smaller indexes per partition**: O(n log n) reduction ‚Üí 18-35% faster Phase 2
2. **Partition pruning**: Query performance 20-30% faster
3. **Parallel index maintenance**: Distributed across partitions

### Actual Results
- Phase 2 degradation: **37-47%** (WORSE than expected 18-35%)
- PAX-no-cluster vs PAX: Negligible difference (1.4%)
- AO had WORST degradation (46.7%), AOCO had BEST (37.2%)

### Possible Root Causes

#### Hypothesis 1: Partition Overhead Exceeds Index Benefits
**Evidence**:
- Index sizes should be 24x smaller per partition
- But overhead of managing 480 partition indexes (5 indexes √ó 4 variants √ó 24 partitions) may exceed savings
- Cloudberry catalog overhead for 96 partition tables

**Test**: Compare monolithic Phase 2 degradation to partitioned

#### Hypothesis 2: Index Maintenance Bottlenecks
**Evidence**:
- Extreme throughput variance (15K to 200K rows/sec in Phase 2)
- Min throughput dropped 9-13x below average
- Suggests periodic stalls (VACUUM, checkpoint, or index bloat)

**Test**: Analyze VACUUM and checkpoint logs during Phase 2

#### Hypothesis 3: No Partition Pruning During INSERT
**Evidence**:
- Partition pruning benefits **queries**, not INSERTs
- INSERTs must still route to correct partition (overhead)
- No pruning benefit for write-heavy workload

**Conclusion**: Partitioning optimizes **reads**, not **writes**

#### Hypothesis 4: Cloudberry-Specific Partition Implementation
**Evidence**:
- Greenplum/Cloudberry uses inheritance-based partitioning
- May have different overhead characteristics than PostgreSQL 10+ declarative partitioning
- `pg_total_relation_size()` returning 0 suggests implementation differences

**Test**: Check Cloudberry docs for partition performance characteristics

---

## 5. Comparison to Expected Targets

### Storage (UNKNOWN - Division by Zero)
| Metric | Expected | Actual | Status |
|--------|----------|--------|--------|
| PAX-no-cluster vs AOCO | +5% overhead | **ERROR** | ‚ùå UNKNOWN |
| PAX clustered vs no-cluster | <5% overhead | **ERROR** | ‚ùå UNKNOWN |

**Action Required**: Re-run with fixed scripts (commit 1c494f7)

---

### Performance

| Metric | Expected | Actual | Status |
|--------|----------|--------|--------|
| Phase 1: PAX vs AO | 85-90% speed | **89.7%** | ‚úÖ MET |
| Phase 2: Degradation | 18-35% | **37-47%** | ‚ùå MISSED |
| Phase 2: PAX vs AO | Competitive | **99.4%** | ‚úÖ EXCEEDED |

**Key Finding**: PAX is competitive with AO in Phase 2 (99.4%), but both suffered worse-than-expected degradation.

---

## 6. Critical Issues Requiring Investigation

### Issue #1: Phase 2 Degradation Worse Than Expected üî¥ CRITICAL

**Severity**: HIGH
**Impact**: Partitioning did NOT achieve expected 18-35% speedup

**Next Steps**:
1. Run monolithic benchmark Phase 2 for comparison
2. Analyze index sizes (should be 24x smaller per partition)
3. Check Cloudberry partition overhead characteristics
4. Profile INSERT operations during Phase 2 for bottlenecks

**Test Command**:
```bash
./scripts/run_streaming_benchmark.py --phase both  # Monolithic baseline
```

---

### Issue #2: Storage Metrics Unavailable üî¥ CRITICAL

**Severity**: HIGH
**Impact**: Cannot validate PAX configuration, storage bloat, or compression

**Root Cause**: `pg_total_relation_size()` returns 0 for parent partitioned tables
**Status**: ‚úÖ **FIXED** in commit 1c494f7 (uses `pg_partition_tree()` now)

**Next Steps**:
1. Re-run Phase 10b and 11b with fixed scripts
2. Analyze storage efficiency
3. Validate PAX bloom filter configuration

**Test Command**:
```bash
psql -f sql/10b_collect_metrics_PARTITIONED.sql
psql -f sql/11b_validate_results_PARTITIONED.sql
```

---

### Issue #3: Extreme Throughput Variance in Phase 2 üü† MODERATE

**Severity**: MODERATE
**Impact**: Unpredictable INSERT performance with 9-13x slowdowns

**Evidence**:
- AO: 15K - 200K rows/sec (1,216% range)
- PAX: 16K - 187K rows/sec (1,073% range)
- Min throughput 9-13x slower than average

**Possible Causes**:
- VACUUM activity
- Checkpoint overhead
- Index bloat accumulation
- Segment rebalancing

**Next Steps**:
1. Analyze `pg_stat_bgwriter` for checkpoint activity
2. Check VACUUM logs during benchmark
3. Monitor `pg_stat_user_indexes` for index bloat

---

### Issue #4: PAX-no-cluster Outperformed PAX Clustered üü† LOW PRIORITY

**Severity**: LOW (1.4% difference)
**Impact**: Clustering may not benefit INSERT workload

**Evidence**:
- PAX-no-cluster: 144,060 rows/sec
- PAX clustered: 142,059 rows/sec
- Difference: 1,001 rows/sec slower (0.7%)

**Analysis**:
- Clustering optimizes **reads** (Z-order locality), not **writes**
- No query workload to benefit from clustering
- Overhead is negligible (1.4%)

**Conclusion**: This is expected - clustering doesn't help INSERT-only benchmarks

---

## 7. Recommendations

### Immediate Actions

1. **Re-run metrics with fixed scripts** (commit 1c494f7):
   ```bash
   psql -f sql/10b_collect_metrics_PARTITIONED.sql
   psql -f sql/11b_validate_results_PARTITIONED.sql
   ```

2. **Run monolithic benchmark for comparison**:
   ```bash
   ./scripts/run_streaming_benchmark.py --phase both
   ```

3. **Analyze index sizes**:
   ```sql
   SELECT
       schemaname || '.' || tablename AS table,
       indexname,
       pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
   FROM pg_stat_user_indexes
   WHERE schemaname = 'cdr' AND tablename LIKE '%partitioned'
   ORDER BY pg_relation_size(indexrelid) DESC;
   ```

4. **Check for index bloat**:
   ```sql
   SELECT * FROM cdr_validation.detect_index_bloat('cdr', 'cdr_ao_partitioned');
   ```

---

### Medium-Term

5. **Adjust bloom filter validation threshold** (false positive):
   - Edit `sql/11b_validate_results_PARTITIONED.sql`
   - Change threshold from >0.95 to >0.85 for cardinality

6. **Add checkpoint monitoring**:
   ```sql
   SELECT * FROM pg_stat_bgwriter;
   ```

7. **Profile Phase 2 bottlenecks**:
   - Enable `log_autovacuum = on`
   - Enable `log_checkpoints = on`
   - Re-run Phase 2 and analyze logs

---

### Long-Term

8. **Test with query workload** to validate partition pruning benefits:
   - Add SELECT queries to benchmark
   - Measure partition pruning effectiveness
   - Compare query performance vs monolithic

9. **Document Cloudberry partition behavior**:
   - `pg_total_relation_size()` returning 0
   - Partition overhead characteristics
   - Best practices for partitioned benchmarks

10. **Create automated comparison script**:
    - Monolithic vs partitioned metrics side-by-side
    - Automated regression detection
    - Performance trend analysis

---

## 8. Summary

### What Worked ‚úÖ
- **Partition routing**: Perfect data distribution across 24 partitions
- **Data integrity**: All variants have identical row counts (29.21M)
- **Phase 1 performance**: PAX at 89.7% of AO (expected)
- **Validation framework**: Detected critical issues (storage metrics, bloom filters)

### What Didn't Work ‚ùå
- **Phase 2 speedup**: 37-47% degradation (vs expected 18-35% improvement)
- **Storage metrics**: Division by zero errors (fixed in commit 1c494f7)
- **Throughput stability**: Extreme variance (15K to 200K rows/sec)

### Critical Questions ‚ùì
1. How does partitioned Phase 2 compare to monolithic?
2. What is causing 9-13x throughput drops in Phase 2?
3. Are smaller partition indexes actually providing benefits?
4. Is Cloudberry partition overhead negating index size reduction?

### Next Steps
1. ‚úÖ **DONE**: Fix division by zero errors (commit 1c494f7)
2. ‚è≠Ô∏è **TODO**: Re-run metrics with fixed scripts
3. ‚è≠Ô∏è **TODO**: Run monolithic benchmark for comparison
4. ‚è≠Ô∏è **TODO**: Investigate Phase 2 throughput variance
5. ‚è≠Ô∏è **TODO**: Document findings in PAX_DEVELOPMENT_TEAM_REPORT.md

---

**Analysis Completed**: November 3, 2025
**Analyst**: Claude (AI Assistant)
**Status**: Awaiting monolithic baseline comparison
